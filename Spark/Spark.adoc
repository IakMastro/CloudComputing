# Apache Spark
:stem:

image:img/Apache_Spark_logo.png[Spark Logo]

Το Apache Spark είναι ένα εναλλακτικό API του Hadoop, το οποίο δουλεύει 100 φόρες πιο γρήγορα, μπορεί να υλοποιηθεί ευκολότερα σε διάφορες γλώσσες, όπως στη Java, στη Scala και στη Python.

## TF-IDF

Το παράδειγμα που μελετήθηκε στο Spark είναι το TF-IDF. Το παράδειγμα του TF μελετήθηκε στο Hadoop. Το IDF είναι ένας εναλλακτίκος αλγόριθμος του DF, ο οποίος μετράει σε πόσα έγγραφα εμφανίζεται η κάθε λέξη, με τη διαφορά ότι βρίσκει την αντίστροφη συχνότητα εγγράφου. Συγκεκριμένα, είναι ένας μετρήτης που βρίσκει πόση πληροφορία μία λέξη παράγει συνολικά σε ένα κείμενο. Το γίνομενο αύτων των δύο επίστρεφει το TF-IDF.

stem:[tf(t, d) = 0.5 + 0.5 * \frac{f_{td}}{max\{f_{t',d}: t' \in d\}}]

stem:[idf(t, D) = log\frac{N}{|\{d \in D : t \in d\}}]

stem:[tfidf(t, d, D) = tf(t, d) * idf(t, D)]

Στην συγκεκριμένη υλοποίηση χρησιμοποίηθηκε η γλώσσα Scala.

### Configuration

Το configuration του Spark είναι το πρώτο βήμα που ολοκληρώνεται. Αυτό γίνετε με τις εξής εντολές.

```scala
val conf = new SparkConf().setAppName("TFIDF").setMaster("local")
```

Πρώτα θα δωσεί ένα όνομα στο εκτελέσιμο Spark πρόγραμμα.

```scala
val spark = SparkSession.builder.config(conf).getOrCreate()

val sc = spark.sparkContext
```

Έπειτα, θα δώσει στον builder του Spark το configuration που εφτιάξε πριν και στην συνέχεια θα επίστρεψει ένα context.

```spark
val input = sc.wholeTextFiles("file://" + System.getProperty("user.dir") + "/data/*")
      .map(file => (file._1.split('/').last, file._2))
val inputDF = spark.createDataFrame(input).toDF("fileNames", "fileText")
```

Τέλος, θα διαβάσει από το path τα αρχεία που θα επεξεργάστουν και θα δημιουργηθεί ένα dataframe από αυτά. Dataframe είναι μία κατανεμημένη συλλόγη από πληροφορίες (Dataset) οργανωμένη σε στήλες, σαν ένα table σε μία βάση δεδομένων με σχεσιακό μοντέλο ή ένα dataframe του Pandas module της Python.

### Υλοποίηση TF

Το πρώτο βήμα μέτα το configuration είναι ο υπολογίσμος του TF.

```scala
val tokenizer = new Tokenizer().setInputCol("fileText").setOutputCol("words")
val wordsData = tokenizer.transform(inputDF)
```

Αυτές οι εντολές είναι το Mapper του Spark και είναι αντιστοίχο του Hadoop mapper. Η διαφορά είναι ότι γίνετε πιο απλά, με ετοίμες συνάρτησεις. Το ίδιο ισχύει και για τον Reducer.

```scala
val hashingTF = new HashingTF().setInputCol("words").setOutputCol("rawFeatures")
val featurizedData = hashingTF.transform(wordsData)
```

Εφόσον έχει χωρίσει τις πληροφορίες σε λέξεις, τότε με την συνάρτηση *HasingTF* μετράει αυτόματα τις λέξεις μέσα στο έγγραφο, βρίσκει τη συχνότητα εμφάνισης τους και το αποθηκεύει στο *rawFeatures*.

### Υλοποίηση IDF

Τρίτο βήμα είναι ο υπολογισμός του IDF. Αυτό γίνετε παρόμοια με το TF με έτοιμες συναρτήσεις.

```scala
val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features")
val idfModel = idf.fit(featurizedData)
```

### Υλοποίηση TFIDF

Τέλος, γίνετε υπολογίσμος του TF-IDF, που ομοίως με τα προηγούμενα παραδείγματα υπάρχουν έτοιμες συναρτήσεις.

```scala
val rescaledData = idfModel.transform(featurizedData)
rescaledData.select("fileNames", "features").show()
```

Στη τελεύταια εντολή, εφόσον βρέθηκαν τα στοιχεία που ζητήθηκαν να βρεθούν, γίνετε select (όπως το select της SQL) και τυπώνει τα αποτελέσματα.

### Ενδεικτικό τρέξιμο

Όπως και στα παραδείγμα του Hadoop, έτσι και εδώ χρησιμοποιήθηκαν τα ίδια δείγματα. Τα αποτελέσματα είναι τα εξής:

```bash
+-----------------+--------------------+
|        fileNames|            features|
+-----------------+--------------------+
|       heroes.txt|(262144,[2325,912...|
|   roundabout.txt|(262144,[14,5765,...|
|  simple_text.txt|(262144,[49304,90...|
|suppers_ready.txt|(262144,[469,535,...|
+-----------------+--------------------+
```
