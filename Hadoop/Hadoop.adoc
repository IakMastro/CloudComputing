# Hadoop

image:img/hadoop.png[Hadoop Logo]

Το Hadoop είναι ένα framework χρήσιμο για την ανάπτυξη κατανεμημένης επεξεργασίας μέσα από ένα cluster υπολογιστών χρησιμοποιώντας απλά προγραμματιστικά μοντέλα. Έχει σχεδιαστεί με τέτοιον τρόπο ώστε να μπορεί να προσαρμόζεται να δουλεύει είτε από έναν server είτε σε χιλιάδες μηχανάκια.

## WCount (TF)

Ένα κλασσικό παράδειγμα είναι το WCount, το όποιο μετράει πόσες φόρες εμφανίζεται κάθε όρος μέσα στα έγγραφα. Στην ανάκτηση πληροφορίας, αυτό ονομάζεται tf. Το MapReduce είναι χωρισμένο σε τρία τμήματα κώδικα. Στην κλάση του mapper, στην κλάση του reducer και τέλος σε έναν runner. Αυτό το μοντέλο ονομάζεται MapReducer.

### Mapper

Το πρώτο στάδιο του hadoop είναι πάντα το Mapper.

```java
public class TFMapper extends MapReduceBase
        implements Mapper<LongWritable, Text, Text, IntWritable> {
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    @Override
    public void map(LongWritable longWritable,
                    Text text,
                    OutputCollector<Text, IntWritable> outputCollector,
                    Reporter reporter) throws IOException {
        var line = text.toString();
        var tokenizer = new StringTokenizer(line);

        while (tokenizer.hasMoreTokens()) {
            word.set(tokenizer.nextToken());
            outputCollector.collect(word, one);
        }
    }
}
```

Το Mapper ουσιαστικά αντιστοιχεί keywords με values, όπως και άλλες υλοποίησης της δομής Map (για παράδειγμα το HashMap της Java, το Dictionary της Python και το JSON της Javascript). Σε αυτή την περίπτωση, αντιστοιχούμενες τιμές είναι ένα Text με ένα IntWritable. Και τα δύο είναι classes του Hadoop.

### Reducer

Το δεύτερο στάδιο του Hadoop είναι το Reducer.

```java
public class TFReducer extends MapReduceBase
        implements Reducer<Text, IntWritable, Text, IntWritable> {
    @Override
    public void reduce(Text text,
                       Iterator<IntWritable> iterator,
                       OutputCollector<Text, IntWritable> outputCollector,
                       Reporter reporter) throws IOException {
        var sum = 0;

        while (iterator.hasNext()) {
            sum += iterator.next().get();
        }

        outputCollector.collect(text, new IntWritable(sum));
    }
}
```

To Reducer διαβάζει κάθε key αυτόματα και παίρνει τις τιμές του, κάνει κάποιους υπολογίσιμους και γράφει στο output αρχείο το τελικό αποτέλεσμα. Στο παράδειγμα του wcount, μετράει τις λέξεις που έγιναν map. Δηλαδή εάν η λέξη "up" εμφανίζεται τρεις φόρες στο κείμενο, θα γράψει στο output "up 3".

### Runner

Ο runner δεν είναι ακριβώς στάδιο του Hadoop, άλλα πιο πολύ μία configuration κλάση, που δηλώνει ο προγραμματιστής στο MapReduce ποιες κλάσεις να χρησιμοποιηθούν, ποιος ο τρόπος εγγραφής στο τελικό έγγραφο, κτλ.
Στην αρχή, γίνεται αρχικοποίηση του configuration. Δηλώνεται ότι το configuration βρίσκεται στην κλάση του TFRunner και το όνομα του job είναι tf.

```java
var conf = new JobConf(TFRunner.class);
conf.setJobName("tf");
```

Στη συνέχεια, δηλώνεται ότι τα δεδομένα που θα γραφτούν στο αρχείο εξόδου θα είναι Text και IntWritable. Αυτά, αντιστοιχίζονται με τον Reducer, αφού αυτός είναι υπεύθυνος για την εγγραφή στο αρχείο εξόδου.

```java
conf.setOutputKeyClass(Text.class);
conf.setOutputValueClass(IntWritable.class);
```

Επιπρόσθετα, δηλώνονται ποιες είναι οι κλάσεις του Mapper και του Reducer, όπως και του Combiner. Στην περίπτωση του WCount ενδέχεται να μη χρησιμεύει το Combiner.

```java
conf.setMapperClass(TFMapper.class);
conf.setCombinerClass(TFReducer.class);
conf.setReducerClass(TFReducer.class);
```

Τέλος, δηλώνονται τα formats των αρχείων εισόδου και εξόδου. Και στις δύο περιπτώσεις, είναι Text, εφόσον φορτώνουν απλά εγγράφου (.txt για παράδειγμα, ή από το hdfs://localhost:9870. Περισσότερα στα ενδεικτικά τρεξίματα).

```java
conf.setInputFormat(TextInputFormat.class);
conf.setOutputFormat(TextOutputFormat.class);
```

Εφόσον, το configuration του Runner είναι έτοιμο, το μόνο που λείπει να γίνει είναι να του δωθεί το path των αρχείων είσοδο και το path του αρχείου εξόδου. Ο ολοκληρωμένος κώδικας του Runner είναι ο εξής:

```java
public class TFRunner {
    public static void main(String[] args) {
        var conf = new JobConf(TFRunner.class);
        conf.setJobName("tf");

        conf.setOutputKeyClass(Text.class);
        conf.setOutputValueClass(IntWritable.class);

        conf.setMapperClass(TFMapper.class);
        conf.setCombinerClass(TFReducer.class);
        conf.setReducerClass(TFReducer.class);

        conf.setInputFormat(TextInputFormat.class);
        conf.setOutputFormat(TextOutputFormat.class);

        var paths = new Path[args.length - 1];
        for (int i = 0; i < paths.length; i++)
            paths[i] = new Path(args[i]);

        FileInputFormat.setInputPaths(conf, paths);
        FileOutputFormat.setOutputPath(conf, new Path(args[paths.length]));

        try {
            JobClient.runJob(conf);
        } catch (IOException e) {
            e.printStackTrace();
            System.err.println("Wrong input/output");
        }
    }
}
```

### Ενδεικτικά τρεξίματα

Για τα ενδεικτικά τρεξίματα, θα χρησιμοποιήσει το εξής απλό αρχείο 6 λέξεων:

```text
hi bye hi three three three
```

Σαφέστατα, στο αρχείο εξόδου, πρέπει να εμφανίζει το hi 2, bye 1 και three 3.
Εκτελώντας το αρχείο, επιστρέφονται τα εξής αποτελέσματα.

```text
bye	1
hi	2
three	3
```

Τα αποτελέσματα του αρχείου είναι σωστά και λογικά.

## DF

Ο αλγόριθμος DF είναι ένας αλγόριθμος της Ανάκτησης Πληροφορίας ο οποίος μετράει σε πόσα έγγραφα εμφανίζεται η κάθε λέξη. Όπως και στο παράδειγμα του WCount, έτσι και εδώ το πρόγραμμα του MapReducer χωρίζεται στα τρία. Ο Mapper είναι ίδιος με τον Mapper του TF.

### Reducer

Ο Reducer παρόλο που θυμίζει τον Reducer του WCount, έχουν μία μικρή διαφορά που αλλάζει ριζικά το πως δουλεύει.

```java
public class DFReducer extends MapReduceBase
        implements Reducer<Text, IntWritable, Text, IntWritable> {
    @Override
    public void reduce(Text text,
                       Iterator<IntWritable> iterator,
                       OutputCollector<Text, IntWritable> outputCollector,
                       Reporter reporter) throws IOException {
        var count = 0;

        while (iterator.hasNext()) {
            count++; // Εδώ είναι η διαφορά
            iterator.next();
        }

        outputCollector.collect(text, new IntWritable(count));
    }
}
```

Ουσιαστικά στον runner, το combiner class επειδή θα τρέξει δεύτερη φόρα, θα πάρει τα δεδομένα από το reducer class και επιστρέψει στο τελικό αρχείο το σωστό αποτέλεσμα.

### Ενδεικτικά τρεξίματα

Για τα ενδεικτικά τρεξίματα, χρησιμοποιήθηκαν οι στίχοι τριών τραγουδιών, συγκεκριμένα το Roundabout των Yes, το Supper's Ready των Genesis και το Heroes του θρυλικού David Bowie.

```text
'n'	1
A	1
All	1
Along	1
And	2
As	1
Bacon	1
Bang,	1
...
```

Αυτά είναι τα οχτώ πρώτα αποτελέσματα που επέστρεψε το MapReduce. Βλέπουμε ότι όλες οι λέξεις εμφανίζονται σε ένα τραγούδι έκτος από το And που εμφανίζεται σε δύο.

## TFI

Το TFI είναι ένας αλγόριθμος της ανάκτησης πληροφορίας, όπου υπολογίζει τη συχνότητα εμφάνισης ενός όρου μέσα στο έγγραφο.

:stem:

stem:[tf("this", d_1) = \frac{1}{5}]

Αυτό σημαίνει ότι μέσα στο έγγραφο d1, το "this" εμφανίζεται μια φόρα στις 5 λέξεις και έχει συχνότητα 0.2.

### Mapper

Στην κλάση TFIMapper, αντιθέτως με τις προηγούμενες Mapper κλάσεις που αναλυθήκαν, είναι αρκετά διαφορετική. Καταρχάς, αυτή τη φόρα αντιστοιχεί Text με DoubleWritable, για τον λόγο ότι γίνεται διαίρεση στον Reducer.
Για να υπολογίζει το tfi θα πρέπει κάπως να κρατάει πόσες λέξεις διάβασε συνολικά ανά αρχείο. Αυτό γίνεται με τον εξής τρόπο:

```java
public static HashMap<String, Integer> wordsCounted = new HashMap<>();
```

Δημιουργείται, δηλαδή, μία HashMap η οποία αντιστοιχεί το όνομα του αρχείου με έναν αριθμό. Αυτό γίνεται σε αυτές τις γραμμές κωδικά.

```java
int count = wordsCounted.getOrDefault(fileName, 0);
wordsCounted.put(fileName, count + 1);
```

Αρχικά, με την εντολή getOrDefault(fileName, 0) ουσιαστικά κοιτάει το key του fileName, το οποίο είναι το αρχείο εισόδου που το παίρνει από αυτή τη γραμμή.

```java
var fileName = ((FileSplit)reporter.getInputSplit()).getPath().getName();
```

Εάν, το key δεν έχει βρεθεί, επειδή είναι η πρώτη φορά που περνάει το αρχείο λογικά, τότε παίρνει την τιμή 0, διαφορετικά παίρνει το value του key στο map. Τέλος, φορτώνει το key στο Map και του αντιστοιχεί την τιμή count + 1. Αυτό, θεωρητικά θα του δώσει την τελική τιμή για το πόσες λέξεις έχει το έγγραφο μέσα του. Επιπλέον, αφού είναι public, σημαίνει ότι μπορεί να το δει και το Reducer αργότερα, με σκοπό να γίνουν σωστά οι υπολογισμοί.
Εφόσον, αναλυθήκαν τα νέα δύσκολα τμήματα του κωδικά, από κάτω ακολουθείται ολόκληρος ο κώδικας.

```java
public class TFIMapper extends MapReduceBase
        implements Mapper<LongWritable, Text, Text, DoubleWritable> {
    private final static DoubleWritable one = new DoubleWritable(1.0f);
    private Text word = new Text();

    // HashMap used to keep track counted words of each file.
    public static HashMap<String, Integer> wordsCounted = new HashMap<>();

    @Override
    public void map(LongWritable longWritable,
                    Text text,
                    OutputCollector<Text, DoubleWritable> outputCollector,
                    Reporter reporter) throws IOException {
        var fileName = ((FileSplit)reporter.getInputSplit()).getPath().getName();
        var line = text.toString();
        var tokenizer = new StringTokenizer(line);

        while (tokenizer.hasMoreTokens()) {
            word.set(tokenizer.nextToken() + " " + fileName );
            outputCollector.collect(word, one);

            // If fileName is not on the map, then get a default value of 0.
            int count = wordsCounted.getOrDefault(fileName, 0);
            wordsCounted.put(fileName, count + 1);
        }
    }
}
```

### Reducer

Ο TFIReducer είναι παρόμοιος με το TFReducer. Υπάρχει μία διαφορά όμως.

```java
public class TFIReducer extends MapReduceBase
        implements Reducer<Text, DoubleWritable, Text, DoubleWritable> {
    @Override
    public void reduce(Text text,
                       Iterator<DoubleWritable> iterator,
                       OutputCollector<Text, DoubleWritable> outputCollector,
                       Reporter reporter) throws IOException {
        var sum = 0.0f;

        while (iterator.hasNext())
            sum += iterator.next().get();

        // Get the words counted in Mapper for the word in question.
        outputCollector.collect(text, new DoubleWritable(sum / TFIMapper.wordsCounted.get(
                text.toString().split(" ")[1]))); // Εδώ είναι η διαφορά
    }
}
```

Επειδή, δεν είναι ακριβώς ξεκάθαρο το πως δουλεύει, ας αναλυθεί τι κάνει η συγκεκριμένη εντολή:

```java
TFIMapper.wordsCounted.get(text.toString().split(" ")[1]);
```

Όπως αναλύθηκε στο Mapper, το wordsCounted.get επιστρέφει την αντιστοιχημένη τιμή του key πίσω για επιπλέον πληροφορία. Επειδή, στο text, το Mapper προσθέτει και το όνομα του αρχείου στην αρχή, με το text.toString().split(" ")[1], αφαιρείται το όνομα του αρχείου, αφού χωρίζει το text αναλόγως του κενού και παίρνει το δεύτερο μέρος του που βρίσκεται η λέξη.

### Ενδεικτικά τρεξίματα

Αρχικά, για να επιβεβαιωθεί ότι δουλεύει σωστά ο αλγόριθμος, θα χρησιμοποιηθεί το αρχείο που είχε χρησιμοποιηθεί στο TF.

```text
hi bye hi three three three
```

Ο θεωρητικός υπολογισμός τους είναι ο εξής:

stem:[tf("hi", d) = \frac{2}{6} = 0.33]

stem:[tf("bye", d) = \frac{1}{6} = 0.16]

stem:[tf("three", d) = \frac{3}{6} = 0.5]

Αφού εκτελεσθεί το αρχείο, για να θεωρηθεί σωστό, θα πρέπει να έχει αυτές τις τιμές.
Στο αρχείο εξόδου, επιστρέφει τις εξής τιμές:

```text
bye simple_text.txt	0.1666666716337204
hi simple_text.txt	0.3333333432674408
three simple_text.txt	0.5
```

Οπότε, μπορεί να σημειωθεί ότι ο αλγόριθμος είναι ορθός.

### Ενδεικτικό τρέξιμο σε μεγάλη κλίμακα δεδομένων

Μεγάλο ενδιαφέρον του αντικείμενου της ανάλυσης δεδομένων είναι να μπορεί ο αλγόριθμός να χρησιμοποιηθεί σε μία μεγάλου όγκου δεδομένων και όχι σε εκείνο το μικρό, ο οποίος αναλύθηκε μόνο για την επιβεβαίωση ορθότητας του αλγορίθμου.
Για άλλη μια φόρα, θα αναλυθούν τα ίδια τρία τραγούδια που αναλύθηκαν και στο DF. Το αρχείο στην έξοδο επέστρεψε τα εξής δεδομένα:

```text
'n' roundabout.txt	0.010695187374949455
A suppers_ready.txt	0.0021030493080615997
All suppers_ready.txt	0.0010515246540307999
Along roundabout.txt	0.002673796843737364
And heroes.txt	0.02631578966975212
And suppers_ready.txt	0.015772869810461998
As suppers_ready.txt	0.0021030493080615997
Bacon suppers_ready.txt	0.0010515246540307999
Bang, suppers_ready.txt	0.0010515246540307999
Better suppers_ready.txt	0.0010515246540307999
British suppers_ready.txt	0.0010515246540307999
But heroes.txt	0.003759398590773344
But suppers_ready.txt	0.0010515246540307999
Call roundabout.txt	0.010695187374949455
...
```

Παρότι οι πληροφορίες που επέστρεψε είναι ορθές, όμως θα μπορούσε να παρατηρήσει κανείς, επιστρέφονται σημεία στίξης, κεφαλαία κτλ που ίσως δε θα έπρεπε να υπήρχαν. Για να λυθεί αυτό το πρόβλημα στον πραγματικό κόσμο, θα πρέπει να καθαριστούν αυτά (με τη χρήση των κατάλληλων βιβλιοθηκών). Έτσι με αποτέλεσμα θα έχουμε μία πιο πραγματική προσέγγιση στην πραγματική τιμή της συχνότητας της λέξεως. Βεβαία, για τη συγκεκριμένη εργασία δεν υπάρχει λόγος να αναλυθεί πολύ, απλώς καλό είναι να αναφέρονται και να σημειώνονται αυτά.

## MaxTFI

Το MaxTFI είναι μία προέκταση του TFI προγράμματος. Σκοπός του είναι να βρίσκει το μέγιστο TFI ένα όρου μέσα στα έγγραφα. Για παράδειγμα, άμα η λέξη "vision" έχει TFI 0.5 στο έγγραφο 1 και 0.6 στο έγγραφο 2, θα πρέπει να κρατάει το TFI του εγγράφου 2.

### Mapper

Ο Mapper του MaxTFI είναι αρκετά ενδιαφέρον επειδή σχεδιάστηκε με το σκεπτικό ότι θα χρησιμοποιήσει το έγγραφο εξόδου του TFI ως το έγγραφο εισόδου.

```java
public class MaxTFIMapper extends MapReduceBase
        implements Mapper<LongWritable, Text, Text, DoubleWritable> {
    @Override
    public void map(LongWritable longWritable,
                    Text text,
                    OutputCollector<Text, DoubleWritable> outputCollector,
                    Reporter reporter) throws IOException {
        var line = text.toString().split("\t");
        outputCollector.collect(new Text(line[0].split(" ")[0]),
                new DoubleWritable(Double.parseDouble(line[1])));
    }
}
```

Ουσιαστικά, σπάει τη γραμμή στα δύο και κρατάει τον όρο και τη συχνότητα TFI του. Δεν υπάρχει ενδιαφέρον στο να κρατηθεί η πληροφορία για το ποιο έγγραφο άνηκε κάποτε ο όρος.

### Reducer

Στην αρχή του reducer έχουμε αυτή την τιμή:

```java
var max = Double.MIN_VALUE;
```

Όπου παίρνει αυτόματα την ελάχιστη τιμή που μπορεί να πάρει ένας αριθμός διπλής ακρίβειας.
Ο υπόλοιπος reducer είναι αρκετά απλός. Κρατάει και καταγράφει μόνο τη μέγιστη συχνότητα TFI που έχει ο όρος σε οποιεσδήποτε έγγραφο.

```java
public class MaxTFIReducer extends MapReduceBase
        implements Reducer<Text, DoubleWritable, Text, DoubleWritable> {
    @Override
    public void reduce(Text text,
                       Iterator<DoubleWritable> iterator,
                       OutputCollector<Text, DoubleWritable> outputCollector,
                       Reporter reporter) throws IOException {
        var max = Double.MIN_VALUE;

        while (iterator.hasNext()) {
            var tfi = iterator.next().get();

            if (tfi > max)
                max = tfi;
        }

        outputCollector.collect(text, new DoubleWritable(max));
    }
}
```

### Ενδεικτικά τρεξίματα

Για τα ενδεικτικά τρεξίματα του MaxTFI, θα χρησιμοποιηθούν τα αποτελέσματα του TFI. Για να υπάρχει σημείο σύγκρισης με τα αποτελέσματα του MaxTFI, σημειώνονται από κάτω.

```text
'n' roundabout.txt	0.010695187374949455
A suppers_ready.txt	0.0021030493080615997
All suppers_ready.txt	0.0010515246540307999
Along roundabout.txt	0.002673796843737364
And heroes.txt	0.02631578966975212
And suppers_ready.txt	0.015772869810461998
As suppers_ready.txt	0.0021030493080615997
Bacon suppers_ready.txt	0.0010515246540307999
Bang, suppers_ready.txt	0.0010515246540307999
Better suppers_ready.txt	0.0010515246540307999
British suppers_ready.txt	0.0010515246540307999
But heroes.txt	0.003759398590773344
But suppers_ready.txt	0.0010515246540307999
Call roundabout.txt	0.010695187374949455
...
```

Να σημειωθεί ότι στον όρο And στο τραγούδι heroes έχει συχνότητα TFI 0.02, ενώ στο supper's ready έχει 0.01, όποτε για να είναι ορθό η υλοποίηση του MaxTFI, θα πρέπει να κρατάει το 0.02 ως τιμή της συχνότητας.

```text
'n'	0.010695187374949455
A	0.0021030493080615997
All	0.0010515246540307999
Along	0.002673796843737364
And	0.02631578966975212
As	0.0021030493080615997
Bacon	0.0010515246540307999
Bang,	0.0010515246540307999
Better	0.0010515246540307999
British	0.0010515246540307999
But	0.003759398590773344
Call	0.010695187374949455
Can't	0.0021030493080615997
Catching	0.002673796843737364
Cause	0.003759398590773344
Churchill	0.0010515246540307999
Coming	0.0010515246540307999
...
```

Όντως, το And έχει συχνότητα 0.02, όποτε ο αλγόριθμος έχει υλοποιηθεί σωστά.
